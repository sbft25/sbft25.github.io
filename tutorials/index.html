---
title: SBFT'25 Tutorials
layout: default
---
<div class="col-md-8 ml-auto mr-auto text-left">

    <div class="section section-team text-left">
        <div class="container">
          <h1 class="title">Tutorials</h1>

          <div class="team">
            <div class="row">
              <div class="team-player">
                <h5 class="text-primary">Testing the Evilness of Large Language Models</h5>
                <h4 class="title">Miguel Romero-Arjona and Aitor Arrieta</h4>
                <p class="text-secondary">SCORELAB, Spain and Mondragon University, Spain</p>
                <p class="text-justify">
                  <strong>Abstract</strong>:
                  Large Language Models (LLMs) are becoming an integral part of our daily lives. But what if they provide dangerous advice—like instructions on poisoning a neighbor? Or if they make wrong assumptions that influence real-world decisions, such as recommending men for leadership roles while relegating women to supportive positions? At first glance, LLMs often appear polite and helpful… but can we uncover their hidden "evilness"?
                  In this tutorial, we will explore practical techniques and tools for testing what we refer to as the evilness of LLMs. Specifically, we will focus on two critical aspects: safety and bias. We will start by introducing the key concepts behind these issues, explaining why they matter and how they manifest in LLM behavior. Then, through hands-on exercises, we will demonstrate how to systematically test LLM safety using our tool ASTRAL, followed by an interactive session on detecting and analyzing bias with our tool suite Meta-Fair.
                  By the end of the tutorial, participants will be equipped with practical skills and tools to automatically test and evaluate the evilness of LLMs.
                </p>
              </div>
            </div>
            
            <!-- <div class="row">
              <div class="team-player">
                <h5 class="text-primary">Title coming soon</h5>
                <h4 class="title">Seongmin Lee</h4>
                <p class="text-secondary">Max Planck Institute for Security and Privacy (MPI-SP), Germany</p>
                <p class="text-justify">
                  <strong>Abstract</strong>:
                  Coming soon
                </p>
              </div>
            </div> -->
            
          </div>
        </div>
      </div>
</div>