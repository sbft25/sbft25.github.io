---
title: SBFT'25 Tutorials
layout: default
---
<div class="col-md-8 ml-auto mr-auto text-left">

    <div class="section section-team text-left">
        <div class="container">
          <h1 class="title">Tutorials</h1>

          <div class="team">
            <div class="row">
              <div class="team-player">
                <h5 class="text-primary">Testing the Evilness of Large Language Models</h5>
                <h4 class="title">Miguel Romero-Arjona and Aitor Arrieta</h4>
                <p class="text-secondary">SCORELAB, Spain and Mondragon University, Spain</p>
                <p class="text-justify">
                  <strong>Abstract</strong>:
                  Large Language Models (LLMs) are becoming an integral part of our daily lives. But what if they provide dangerous advice—like instructions on poisoning a neighbor? Or if they make wrong assumptions that influence real-world decisions, such as recommending men for leadership roles while relegating women to supportive positions? At first glance, LLMs often appear polite and helpful… but can we uncover their hidden "evilness"?
                  In this tutorial, we will explore practical techniques and tools for testing what we refer to as the evilness of LLMs. Specifically, we will focus on two critical aspects: safety and bias. We will start by introducing the key concepts behind these issues, explaining why they matter and how they manifest in LLM behavior. Then, through hands-on exercises, we will demonstrate how to systematically test LLM safety using our tool ASTRAL, followed by an interactive session on detecting and analyzing bias with our tool suite Meta-Fair.
                  By the end of the tutorial, participants will be equipped with practical skills and tools to automatically test and evaluate the evilness of LLMs.
                </p>
              </div>
            </div>
            
            <div class="row">
              <div class="team-player">
                <h5 class="text-primary">The Magic of Statistics for Software Testing: How to Foresee the Unseens</h5>
                <h4 class="title">Seongmin Lee</h4>
                <p class="text-secondary">Max Planck Institute for Security and Privacy (MPI-SP), Germany</p>
                <p class="text-justify">
                  <strong>Abstract</strong>:
                  Ensuring software correctness is essential as software increasingly governs critical aspects of modern life. Formal methods of program verification, while powerful, often struggle with scalability when faced with the complexity of modern systems. Meanwhile, software testing—finding defects by executing the program—is practical but inherently incomplete, as it inevitably misses certain behaviors, i.e., the “unseens,” leaving critical gaps in verification.
                  In this tutorial, I illuminate the transformative potential of statistical methods in addressing these challenges, with a particular focus on residual risk analysis. Residual risk analysis quantifies the likelihood of undiscovered bugs remaining in the software after testing by estimating the probability of finding a new, previously unseen bug in the next test input.
                  We will begin by demonstrating how statistical estimators can assess residual risk using records from software testing—such as code coverage data—through a hands-on example. The tutorial then explores several advanced extensions to adapt residual risk analysis for more realistic testing scenarios. By the end of this session, participants will gain a deeper understanding of how statistical thinking can provide actionable insights into the unseen behaviors of software systems, ultimately making testing more accountable, transparent, and efficient.
                </p>
              </div>
            </div>
            
          </div>
        </div>
      </div>
</div>